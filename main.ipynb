{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "886ebd7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9, 40, 601)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# # input_question = \"I'm going to classify the following chinese medicine spectral data.\"\n",
    "# input_question = \"I'm going to anomaly detect the chinese medicine spectral data.\"\n",
    "# data = np.load(r'data\\CN_medicine\\cnml.npy')\n",
    "\n",
    "# input_question = \"I'm going to classify the following Citri Reticulatae Pericarpium spectral data.\"\n",
    "# # input_question = \"I'm going to anomaly detect the Citri Reticulatae Pericarpium spectral data.\"\n",
    "# data = np.load(r'data\\Chenpi\\chenpi.npy')\n",
    "\n",
    "input_question = \"I'm going to classify the following milk spectral data.\"\n",
    "# input_question = \"I'm going to anomaly detect the following milk spectral data.\"\n",
    "data = np.load(r'data\\milk\\milk_data.npy')\n",
    "\n",
    "# input_question = \"I'm going to predict the waste water spectral data quality of COD.\"\n",
    "# data = np.load(r'data\\H2O\\H2Odata_js.npy')\n",
    "# Y = np.load(r'data\\H2O\\H2Olabel_js.npy')\n",
    "\n",
    "\n",
    "# input_question = \"I'm going to predict the fat content in meat sample.\"\n",
    "# X = np.load(r'data\\tecator\\tecator_data.npy')\n",
    "# Y = np.load(r'data\\tecator\\tecator_label.npy')\n",
    "# data=X.reshape(1, X.shape[0], -1)\n",
    "# Y = Y.reshape(1, 215, 1)\n",
    "\n",
    "\n",
    "# input_question = \"I'm going to predict the protein content in corn samples.\"\n",
    "# data = np.load(r'data\\corn\\corn_data.npy')\n",
    "# Y = np.load(r'data\\corn\\protein_label.npy')\n",
    "# data=data.reshape(1, data.shape[0], -1)\n",
    "# Y = Y.reshape(1, Y.shape[1], 1)\n",
    "# regression_label = Y\n",
    "\n",
    "print(data.shape)\n",
    "\n",
    "\n",
    "API = \"sk-25924755df8b4a5e900e4eec74caf179\"\n",
    "base_url = \"https://dashscope.aliyuncs.com/compatible-mode/v1\"\n",
    "\n",
    "## GPT ds API\n",
    "model_API =  \"sk-GodYd4JhmouwdGxgoRJAmlFKvbyImUubriA0ZVgW7faNhOnl\"\n",
    "model_url = \"https://jeniya.top/v1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1a088187",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Research object: milk\n",
      "Task type: classification\n"
     ]
    }
   ],
   "source": [
    "import Entity_extraction\n",
    "extracted = Entity_extraction.extract_entities_and_task(input_question, API, base_url, model_name=\"qwen-plus\")\n",
    "\n",
    "research_object = extracted['research_object']\n",
    "task_type       = extracted['task_type']\n",
    "\n",
    "print(\"Research object:\", research_object)\n",
    "print(\"Task type:\", task_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "878e17bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paper_Name: Milk Classification of NIR spectra using Principal Component Analysis in Python\n",
      "Relevant Scores: 5.948412985878816\n",
      "Paper_Name: Identification of the animal origin of milk via Laser-Induced Breakdown Spectroscopy\n",
      "Relevant Scores: 5.210590843908991\n",
      "Paper_Name: Novel prediction models for hyperketonemia using bovine milk Fourier-transform infrared spectroscopy\n",
      "Relevant Scores: 4.122371872440508\n",
      "[{'paper_name': 'Milk Classification of NIR spectra using Principal Component Analysis in Python', 'preprocessing_method': 'SG', 'feature_extracting_method': 'PCA'}, {'paper_name': 'Identification of the animal origin of milk via Laser-Induced Breakdown Spectroscopy', 'preprocessing_method': 'Feature-wise scaling', 'feature_extracting_method': 'Entire LIBS spectra'}, {'paper_name': 'Novel prediction models for hyperketonemia using bovine milk Fourier-transform infrared spectroscopy', 'preprocessing_method': 'scaling and centering data (Î¼=0, Ïƒ^2=1), Synthetic Minority Oversample Technique with Edited Nearest Neighbors undersampling (SMOTEENN)', 'feature_extracting_method': 'one-dimensional convolutional neural network (1DCNN) feature extraction'}]\n"
     ]
    }
   ],
   "source": [
    "import Retrieval\n",
    "\n",
    "json_path = \"./structured_papers1.json\"\n",
    "\n",
    "papers_list = Retrieval.load_papers_from_json(json_path)\n",
    "\n",
    "bm25_index, tokenized_names = Retrieval.build_bm25_index(papers_list)\n",
    "top_k = 3\n",
    "\n",
    "matched_info = Retrieval.search_papers_with_bm25(\n",
    "    papers=papers_list,\n",
    "    bm25=bm25_index,\n",
    "    tokenized_paper_names=tokenized_names,\n",
    "    query=research_object,\n",
    "    top_k=top_k\n",
    ")\n",
    "\n",
    "print(matched_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ba07732a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Milk Classification of NIR spectra using Principal Component Analysis in Python': {'preprocessing': ['savitzky_golay_smoothing'], 'features': ['pca_feature_extraction']}, 'Identification of the animal origin of milk via Laser-Induced Breakdown Spectroscopy': {'preprocessing': [], 'features': []}, 'Novel prediction models for hyperketonemia using bovine milk Fourier-transform infrared spectroscopy': {'preprocessing': [], 'features': []}}\n",
      "Selected (majority + fallback): {'preprocessing': ['savitzky_golay_smoothing'], 'features': ['pca_feature_extraction']}\n",
      "(9, 40, 601)\n",
      "Feature 'pca_feature_extraction' shape: (9, 40, 5)\n"
     ]
    }
   ],
   "source": [
    "import Agent\n",
    "\n",
    "Default_mode = True\n",
    "\n",
    "if Default_mode:\n",
    "    agent = Agent.SpectralAgent(\n",
    "        api_key=API,\n",
    "        base_url=base_url,\n",
    "        model_name=\"qwen-plus\",\n",
    "        regression_label=Y if task_type == 'regression' else None,\n",
    "    )\n",
    "\n",
    "    methods_map = agent.decide_methods_per_paper(matched_info)\n",
    "    print(methods_map)\n",
    "\n",
    "    paper_order = [m['paper_name'] for m in matched_info] \n",
    "\n",
    "    # 1) Standard usage: majority voting among top K; if tied or zero votes, fall back to the first \"complete\" paper among top K\n",
    "    selected = agent.select_methods(\n",
    "        methods_map,\n",
    "        k=3,\n",
    "        paper_order=paper_order,\n",
    "        require_both=True,   # Require both \"preprocessing + features\"\n",
    "    )\n",
    "    print(\"Selected (majority + fallback):\", selected)\n",
    "\n",
    "\n",
    "else:\n",
    "# Free selection (by index)\n",
    "    selected_by_idx = agent.select_methods(\n",
    "        methods_map,\n",
    "        k=3,\n",
    "        paper_order=paper_order,\n",
    "        prefer_paper=2,       # Pass 2 to select the 2nd paper; pass 1 or 0 to indicate 1-based/0-based indexing for the 1st paper\n",
    "        prefer_strict=True,\n",
    "        require_both=True,\n",
    "    )\n",
    "    print(\"Selected (prefer by index):\", selected_by_idx)\n",
    "\n",
    "# Process data and extract features using selected methods\n",
    "processed_data, features = agent.process_with_methods(data, selected)\n",
    "print(processed_data.shape)\n",
    "for fn, arr in features.items():\n",
    "    print(f\"Feature '{fn}' shape:\", arr.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "76758dd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples: 216\n",
      "Validation samples: 72, labels: ['5', '1', '6', '5', '2', '2', '4', '7', '4', '9', '8', '8', '3', '9', '1', '2', '2', '9', '4', '7', '7', '9', '9', '6', '6', '5', '3', '2', '6', '9', '7', '3', '4', '8', '8', '4', '8', '5', '2', '8', '1', '9', '6', '8', '5', '6', '1', '7', '1', '7', '1', '3', '7', '3', '4', '2', '5', '4', '6', '3', '5', '5', '7', '9', '2', '6', '3', '3', '1', '8', '1', '4']\n",
      "Test samples: 72, labels: ['2', '2', '8', '8', '1', '6', '4', '6', '1', '9', '6', '5', '9', '7', '2', '5', '8', '2', '9', '7', '1', '9', '6', '1', '6', '3', '3', '6', '1', '3', '9', '4', '3', '5', '1', '5', '7', '3', '1', '2', '3', '8', '3', '2', '4', '9', '2', '5', '4', '5', '8', '7', '3', '7', '4', '9', '8', '7', '2', '8', '9', '6', '6', '4', '5', '4', '1', '7', '5', '8', '7', '4']\n"
     ]
    }
   ],
   "source": [
    "from dataset import CLS_Dataset, REG_Dataset, ANO_Dataset\n",
    "from dataset_config import resolve_dataset_config\n",
    "\n",
    "split_ratio = [0.6, 0.2, 0.2]\n",
    "\n",
    "cfg = resolve_dataset_config(research_object)\n",
    "labels = cfg[\"labels\"]\n",
    "random_seed = cfg[\"seed\"]\n",
    "\n",
    "input = list(features.values())[0]\n",
    "input_len = input.shape[0] * input.shape[1]\n",
    "if task_type == 'regression':\n",
    "    input_len = data.shape[1]\n",
    "    total_samples = 100 if input_len > 100 else input_len\n",
    "else:\n",
    "    total_samples = input_len\n",
    "\n",
    "if task_type == 'classification':\n",
    "    dataset = CLS_Dataset(\n",
    "        feature=input,\n",
    "        labels=labels,\n",
    "        total_samples=total_samples,\n",
    "        split_ratio=split_ratio,\n",
    "        random_seed=random_seed\n",
    "    )\n",
    "    dataset.summary()\n",
    "    train_data = dataset.train_data\n",
    "    val_data   = dataset.val_data\n",
    "    test_data  = dataset.test_data\n",
    "    true_labels_val  = dataset.true_labels_val\n",
    "    true_labels_test = dataset.true_labels_test\n",
    "\n",
    "elif task_type == 'regression':\n",
    "    dataset = REG_Dataset(\n",
    "        X=input,\n",
    "        Y=Y,\n",
    "        total_samples=total_samples,\n",
    "        split_ratio=split_ratio,\n",
    "        random_seed=random_seed,\n",
    "    )\n",
    "    dataset.summary()\n",
    "    train_data = dataset.train_data\n",
    "    val_data   = dataset.val_data\n",
    "    test_data  = dataset.test_data\n",
    "    y_val_true  = dataset.y_val_true\n",
    "    y_test_true = dataset.y_test_true\n",
    "\n",
    "elif task_type == 'anomaly detection':\n",
    "    ds = ANO_Dataset(\n",
    "        X=input,\n",
    "        labels=labels,\n",
    "        total_samples=total_samples,\n",
    "        split_ratio=split_ratio,\n",
    "        normal_class=0,\n",
    "        random_seed=random_seed,\n",
    "    )\n",
    "    ds.summary()\n",
    "    train_data = ds.train_data\n",
    "    val_data   = ds.val_data\n",
    "    test_data  = ds.test_data\n",
    "    y_val      = ds.y_val\n",
    "    y_test     = ds.y_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "62effc16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Initial test set evaluation ===\n",
      "Prompt tokens: 16605, Completion tokens: 148\n",
      "raw_model_output: [2,2,8,8,1,6,3,6,1,9,6,5,9,7,2,5,8,2,9,7,1,9,6,1,6,3,3,6,1,3,9,4,3,5,1,5,7,3,1,2,3,8,3,2,4,9,2,5,4,5,8,7,3,7,4,9,8,7,2,8,9,7,6,6,4,5,4,1,7,5,8,7,4]\n",
      "parsed_preds: ['2', '2', '8', '8', '1', '6', '3', '6', '1', '9', '6', '5', '9', '7', '2', '5', '8', '2', '9', '7', '1', '9', '6', '1', '6', '3', '3', '6', '1', '3', '9', '4', '3', '5', '1', '5', '7', '3', '1', '2', '3', '8', '3', '2', '4', '9', '2', '5', '4', '5', '8', '7', '3', '7', '4', '9', '8', '7', '2', '8', '9', '7', '6', '6', '4', '5', '4', '1', '7', '5', '8', '7']\n",
      "true_labels_test: ['2', '2', '8', '8', '1', '6', '4', '6', '1', '9', '6', '5', '9', '7', '2', '5', '8', '2', '9', '7', '1', '9', '6', '1', '6', '3', '3', '6', '1', '3', '9', '4', '3', '5', '1', '5', '7', '3', '1', '2', '3', '8', '3', '2', '4', '9', '2', '5', '4', '5', '8', '7', '3', '7', '4', '9', '8', '7', '2', '8', '9', '6', '6', '4', '5', '4', '1', '7', '5', '8', '7', '4']\n",
      "wrong indices: [6, 61, 63, 64, 65, 66, 67, 68, 69, 70, 71]\n",
      "Test accuracy: 84.72%\n"
     ]
    }
   ],
   "source": [
    "## Single turn generation\n",
    "from Generate_single import SpectrumCLS, SpectrumReg, SpectrumAno\n",
    "\n",
    "if task_type == \"classification\":\n",
    "    agent_cls = SpectrumCLS(\n",
    "        train_data=train_data,\n",
    "        test_data=test_data,\n",
    "        true_labels_test=true_labels_test,\n",
    "        api_key=model_API,\n",
    "        base_url=model_url,\n",
    "        model=\"gpt-5-chat-latest\",\n",
    "        #model = \"qwen-turbo-latest\",\n",
    "    )\n",
    "    agent_cls.run()\n",
    "\n",
    "elif task_type == \"regression\":\n",
    "    agent_reg = SpectrumReg(\n",
    "        dataset= dataset,\n",
    "        api_key=model_API,\n",
    "        base_url=model_url,\n",
    "        model=\"gpt-5-chat-latest\",\n",
    "        #model = \"qwen-plus-latest\",\n",
    "    )\n",
    "    agent_reg.run()\n",
    "\n",
    "elif task_type == 'anomaly detection':\n",
    "    agent_ano = SpectrumAno(\n",
    "        dataset = ds,\n",
    "        api_key=model_API,\n",
    "        base_url=model_url,\n",
    "        model=\"gpt-5-chat-latest\",\n",
    "        #model = \"qwen-plus-latest\",\n",
    "    )\n",
    "    agent_ano.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b41cbc65",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Conda\\envs\\pytorch\\Lib\\site-packages\\torch\\utils\\_pytree.py:185: FutureWarning: optree is installed but the version is too old to support PyTorch Dynamo in C++ pytree. C++ pytree support is disabled. Please consider upgrading optree using `python3 -m pip install --upgrade 'optree>=0.13.0'`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ML][SVM] best_params={'svc__C': 100, 'svc__gamma': 0.01, 'svc__kernel': 'rbf'}, CV_best_acc=1.0000, Test_acc=1.0000\n",
      "[ML][KNN] best_params={'knn__n_neighbors': 7, 'knn__p': 1, 'knn__weights': 'distance'}, CV_best_acc=0.9400, Test_acc=0.9306\n",
      "[ML][RandomForest] best_params={'max_depth': 10, 'min_samples_split': 2, 'n_estimators': 100}, CV_best_acc=0.9537, Test_acc=0.9722\n",
      "[NN-CLS] Early stopped at epoch 12, best val_acc=0.9583\n",
      "[NN-CLS] Early stopped at epoch 23, best val_acc=1.0000\n",
      "SVM: Accuracy = 100.00%\n",
      "KNN: Accuracy = 93.06%\n",
      "RandomForest: Accuracy = 97.22%\n",
      "CNN1D: Accuracy = 91.67%\n",
      "Transformer: Accuracy = 97.22%\n"
     ]
    }
   ],
   "source": [
    "## Other models\n",
    "\n",
    "from other_models import ClassificationModelPipeline,RegressionModelPipeline,AnomalyModelPipeline\n",
    "\n",
    "if task_type =='classification':\n",
    "    cls_pipe = ClassificationModelPipeline(train_data, test_data, val_data=val_data, early_stop_patience=5, cv_folds=5)\n",
    "    cls_results = cls_pipe.train_and_evaluate(nn_epochs=50, batch_size=32)\n",
    "\n",
    "elif task_type =='regression': \n",
    "    reg_pipe = RegressionModelPipeline(train_data, test_data, val_data=val_data, early_stop_patience=5, cv_folds=5)\n",
    "    reg_results = reg_pipe.train_and_evaluate(nn_epochs=50, batch_size=32)\n",
    "    \n",
    "elif task_type == 'anomaly detection':\n",
    "    ano_pipe = AnomalyModelPipeline(train_data, test_data, val_data=val_data, early_stop_patience=5, cv_folds=5)\n",
    "    ano_results = ano_pipe.train_and_evaluate(epochs_ae=50, batch_size=32)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
